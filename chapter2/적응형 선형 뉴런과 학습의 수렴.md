# 적응형 선형 뉴런과 학습의 수렴



## 적응형 선형 뉴런 (ADAptive LInear NEuron, ADALINE)



### 특징

![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/2%EC%9E%A5_algorithm_img.png)



- 가중치를 업데이트할때 선형 활성화 함수를 사용한다. 이는 퍼셉트론과 아달린의 가장 큰 차이점이다.
  $$
  \phi(w^T x)=w^Tx\\
  $$

- 최종 예측을 만드는 데 임계 함수를 사용하는 것은 변함이 없다.

- 아달린 알고리즘은 진짜 클래스 레이블과 선형 활성화 함수의 실수 출력 값의 차이로 오차를 계산하여 가중치를 업데이트 한다.



## 경사 하강법으로 비용 함수 최소화



### 아달린의 비용 함수 정의

- 출력과 진짜 클래스 사이의 차이를 제곱한 후 모두 더한다. (Sum of Squared Errors, SSE)
  $$
  J(w)={1 \over 2}\sum \left( y^{(i)} - \phi \left( z^{(i)} \right) \right)^2
  $$

  > 함수의 경사를 간소화하기 위해 를 추가.

- 선형 활성화 함수는 기울기가 존재하기 때문에 미분 가능하다. 또한 진짜 클래스 레이블이 출력보다 항상 크기 때문에 비용 함수 J는 항상 아래로 볼록한 함수이며 최소값을 갖는다. 즉 비용 함수를 최소화 하는 가중치를 찾을 수 있다.

  

  ![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/2%EC%9E%A5_%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95.png)

  

- gradient(경사)가 존재하기 때문에 비용 함수 J(w)의 gradient 반대 방향(반대 부호)으로 조금 씩 가중치를 업데이트 한다.
  $$
  w := w+\Delta w
  $$

  > 여기서 $$\Delta w$$는 음수 gradient에 학습률 $$\eta$$를 곱한 것으로 정의한다.
  > $$
  > \Delta w = -\eta \bigtriangledown J(w)
  > $$
  > 

  
  
- 비용 함수의 gradient(기울기)를 구하기 위해 비용 함수 J를 편미분 하고, 가중치 변화량은 학습률에 (-)gradient를 곱한다.
  
  
  $$
\Delta w_j = - \eta \sum_i \left( y^{(i)} - \phi \left( z^{(i)}\right) \right)x_j^{(i)}
  $$
  
- 모든 가중치를 동시에 업데이트 시켜 규칙을 정한다.
  $$
  w:=w+\Delta w
  $$

- 아달린의 활성화 함수는 퍼셉트론과 다르게 실수이다.



### 파이썬으로 아달린 구현

> 퍼셉트론과 아달린은 매우 유사하기 때문에 서로의 차이점을 알아본다.

- 퍼셉트론의 데이터 학습

  퍼셉트론의 데이터 학습의 특징은 훈련 데이터(X)와 타깃(y)를 zip()을 통해 서로의 리스트의 같은 인덱스끼리 슬라이싱 한 후 for문을 통해 가중치를 업데이트 한다. 즉 입력 데이터의 결과값을 순차적으로 업데이트하고 오차 또한 순차적으로 구해진다.

![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/2%EC%9E%A5_percept_training_code.png)



- 아달린의 데이터 학습

  아달린의 데이터 학습의 특징은 모든 입력 데이터의 결과값을 1 iter에 한번에 구한 뒤 입력 레이블과의 오차도 구한다. 다시 말해 데이터는 한번에 들어간다.

![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/2%EC%9E%A5_adaline_training_code.png)



- epoch과 error의 관계


![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/2%EC%9E%A5_epoch10.png)



![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/2%EC%9E%A5_epoch100.png)



![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/2%EC%9E%A5_epoch1000.png)



- 아달린의 문제점

  아달린은 비용 함수를 SSE로 정의 하여 이차식으로 만든 후 미분 가능하게 하는 것이 특징이다. 하지만 이렇게 할 경우 학습률과 epoch수를 잘 조절해야 한다. 만약 학습률과 epoch수를 잘 조절하지 못한다면 아무리 학습을 해도 가중 계수를 수렴시키지 못하거나 SSE값이 뜬금없는 값으로 튀어버릴 수 있다. 

  

  ![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/2%EC%9E%A5_proper_training.png)





## 특성 스케일을 조정하여 경사 하강법 결과 향상

> 경사 하강법은 특성 스케일을 조정하여 혜택을 볼 수 있다.



### 표준화(standardization) 

> 표준화는 각 특성의 평균을 0으로, 특성의 표준 편차를 1로 만든다.

- 표준화를 하기 위해서 필요한 공식은 
  $$
  x_{j}{'} = {x_j - \mu _j \over \sigma _j}
  $$



- 표준화의 효과

![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/2%EC%9E%A5_scaling.png)



## 대규모 머신 러닝과 확률적 경사 하강법

> 위에서 배운 배치 경사 하강법은 모든 샘플에 대해 누적된 오차의 합을 기반으로 가중치를 업데이트하지만 확률적 경사 하강법은 각 훈련 샘플에 대해 조금씩 가중치를 업데이트 한다.
>
> 샘플 데이터가 많으면 많을 수록 확률적 경사 하강법의 효율이 배치 경사 하강법보다 증가한다. (속도가 빠르다)



- gradient가 하나의 훈련 샘플을 기반으로 계산되어 오차의 궤적이 어지럽다.
- 하나의 훈련 샘플을 기반으로 하기 때문에 훈련 샘플의 순서를 무작위하게 주입하는 것이 중요하다.
- 훈련 샘플이 순환하는것을 막기 위해 epoch마다 훈련 세트를 섞어준다.
- 확률적 경사 하강법은 전역 최솟값에 도달하지는 못하지만 그에 매우 가까운 지역으로 근접한다.



![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/2%EC%9E%A5_%ED%99%95%EB%A5%A0%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95.png)

- 온라인 학습(online learning)으로 사용할 수 있기 때문에 새로운 훈련 데이터가 도착하는 대로 훈련이 된다.
- 배치 경사 하강법과 확률적 경사 하강법 사이의 절충점으로 미니 배치 학습(mini-batch learning)이 있다.

![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/2%EC%9E%A5_minibatch.png)



### 파이썬으로 확률 경사 하강법 구현

![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/2%EC%9E%A5_%ED%95%99%EC%8A%B5code.png)

- 가중치를 다시 초기화 하지 않게 하여 각 샘플마다 가중치를 업데이트 한다.
- 각 훈련 데이터가 샘플이 되기 때문에 훈련 데이터를 섞는 작업이 필요하다.



![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/2%EC%9E%A5_stochastic.png)

- epoch수가 그리 많지 않음에도 불구하고 error가 빠르게 감소한다. 따라서 학습 훈련 세트를 많이 반복하지 않아도 되기 때문에 데이터 샘플이 많은 학습에서 유리하다.