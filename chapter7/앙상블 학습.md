# 앙상블 학습

![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/7장_앙상블학습_1.jpg)

![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/7장_앙상블학습_2.jpg)



### 다수결 투표 분류기 구현하기

```python
from sklearn.base import BaseEstimator
from sklearn.base import ClassifierMixin
from sklearn.preprocessing import LabelEncoder
from sklearn.externals import six
from sklearn.base import clone
from sklearn.pipeline import _name_estimators
import numpy as np
import operator


class MajorityVoteClassifier(BaseEstimator, 
                             ClassifierMixin):
    """다수결 투표 앙상블 분류기

    매개변수
    ----------
    classifiers : 배열 타입, 크기 = [n_classifiers]
      앙상블에 사용할 분류기

    vote : str, {'classlabel', 'probability'}
      기본값: 'classlabel'
      'classlabel'이면 예측은 다수인 클래스 레이블의 인덱스가 됩니다.
      'probability'이면 확률 합이 가장 큰 인덱스로 
      클래스 레이블을 예측합니다(보정된 분류기에 추천합니다).

    weights : 배열 타입, 크기 = [n_classifiers]
      선택사항, 기본값: None
      `int` 또는 `float` 값의 리스트가 주어지면 분류기가 이 중요도로 가중치됩니다.
      `weights=None`이면 동일하게 취급합니다.

    """
    def __init__(self, classifiers, vote='classlabel', weights=None):

        self.classifiers = classifiers
        self.named_classifiers = {key: value for key, value
                                  in _name_estimators(classifiers)}
        self.vote = vote
        self.weights = weights

    def fit(self, X, y):
        """분류기를 학습합니다.

        매개변수
        ----------
        X : {배열 타입, 희소 행렬},
            크기 = [n_samples, n_features]
            훈련 샘플 행렬

        y : 배열 타입, 크기 = [n_samples]
            타깃 클래스 레이블 벡터

        반환값
        -------
        self : 객체

        """
        if self.vote not in ('probability', 'classlabel'):
            raise ValueError("vote는 'probability' 또는 'classlabel'이어야 합니다."
                             "; (vote=%r)이 입력되었습니다."
                             % self.vote)

        if self.weights and len(self.weights) != len(self.classifiers):
            raise ValueError('분류기 개수와 가중치 개수는 동일해야 합니다.'
                             '; %d개의 가중치와, %d개의 분류기가 입력되었습니다.'
                             % (len(self.weights), len(self.classifiers)))

        # self.predict 메서드에서 np.argmax를 호출할 때 
        # 클래스 레이블이 0부터 시작되어야 하므로 LabelEncoder를 사용하여 숫자로 변경.
        self.lablenc_ = LabelEncoder()
        self.lablenc_.fit(y)
        self.classes_ = self.lablenc_.classes_
        self.classifiers_ = []
        for clf in self.classifiers:
            fitted_clf = clone(clf).fit(X, self.lablenc_.transform(y))
            self.classifiers_.append(fitted_clf)
        return self

    def predict(self, X):
        """X에 대한 클래스 레이블을 예측합니다.

        매개변수
        ----------
        X : {배열 타입, 희소 행렬},
            크기 = [n_samples, n_features]
            샘플 데이터 행렬

        반환값
        ----------
        maj_vote : 배열 타입, 크기 = [n_samples]
            예측된 클래스 레이블
            
        """
        if self.vote == 'probability':
            maj_vote = np.argmax(self.predict_proba(X), axis=1)
        else:  # 'classlabel' 투표

            #  clf.predict 메서드를 사용해 결과를 모읍니다.
            predictions = np.asarray([clf.predict(X)
                                      for clf in self.classifiers_]).T

            maj_vote = np.apply_along_axis(
                                      lambda x:
                                      np.argmax(np.bincount(x,
                                                weights=self.weights)),
                                      axis=1,
                                      arr=predictions)
        maj_vote = self.lablenc_.inverse_transform(maj_vote)
        return maj_vote

    def predict_proba(self, X):
        """X에 대한 클래스 확률을 예측합니다.

        매개변수
        ----------
        X : {배열 타입, 희소 행렬},
            크기 = [n_samples, n_features]
            n_samples는 샘플의 개수이고 n_features는 특성의 개수인
            샘플 데이터 행렬

        반환값
        ----------
        avg_proba : 배열 타입,
            크기 = [n_samples, n_classes]
            샘플마다 가중치가 적용된 클래스의 평균 확률

        """
        probas = np.asarray([clf.predict_proba(X)
                             for clf in self.classifiers_])
        avg_proba = np.average(probas, axis=0, weights=self.weights)
        return avg_proba

    def get_params(self, deep=True):
        """GridSearch를 위해서 분류기의 매개변수 이름을 반환합니다"""
        if not deep:
            return super(MajorityVoteClassifier, self).get_params(deep=False)
        else:
            out = self.named_classifiers.copy()
            for name, step in six.iteritems(self.named_classifiers):
                for key, value in six.iteritems(step.get_params(deep=True)):
                    out['%s__%s' % (name, key)] = value
            return out
```





### 다수결 투표 방식을 사용하여 예측 만들기

- Iris 데이터를 사용하여 MajorityClassifier 클래스를 적용시킬 때 이중분류를 사용하기 위해 

  Iris-versicolor와 Iris-virginica 클래스에 해당하는 샘플만 사용한다.

- 데이터 샘플을 50%를 훈련 데이터로, 나머지 50%를 테스트 데이터로 나눈다.

  ```python
  from sklearn import datasets
  from sklearn.preprocessing import StandardScaler
  from sklearn.preprocessing import LabelEncoder
  from sklearn.model_selection import train_test_split
  
  iris = datasets.load_iris()
  X, y = iris.data[50:, [1, 2]], iris.target[50:]
  le = LabelEncoder()
  y = le.fit_transform(y)
  
  X_train, X_test, y_train, y_test =\
         train_test_split(X, y, 
                          test_size=0.5, 
                          random_state=1,
                          stratify=y)
  ```

- 각 분류기를 앙상블로 묶기 전에 훈련 세트에서 10-겹 교차 검증으로 성능평가

  ```python
  import numpy as np
  from sklearn.linear_model import LogisticRegression
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.neighbors import KNeighborsClassifier 
  from sklearn.pipeline import Pipeline
  from sklearn.model_selection import cross_val_score
  
  clf1 = LogisticRegression(solver='liblinear',
                            penalty='l2', 
                            C=0.001,
                            random_state=1)
  
  clf2 = DecisionTreeClassifier(max_depth=1,
                                criterion='entropy',
                                random_state=0)
  
  clf3 = KNeighborsClassifier(n_neighbors=1,
                              p=2,
                              metric='minkowski')
  
  pipe1 = Pipeline([['sc', StandardScaler()],
                    ['clf', clf1]])
  pipe3 = Pipeline([['sc', StandardScaler()],
                    ['clf', clf3]])
  
  clf_labels = ['Logistic regression', 'Decision tree', 'KNN']
  
  print('10-겹 교차 검증:\n')
  for clf, label in zip([pipe1, clf2, pipe3], clf_labels):
      scores = cross_val_score(estimator=clf,
                               X=X_train,
                               y=y_train,
                               cv=10,
                               scoring='roc_auc')
      print("ROC AUC: %0.2f (+/- %0.2f) [%s]"
            % (scores.mean(), scores.std(), label))
  ```

  - 결과

    ```
    10-겹 교차 검증:
    
    ROC AUC: 0.92 (+/- 0.15) [Logistic regression]
    ROC AUC: 0.87 (+/- 0.18) [Decision tree]
    ROC AUC: 0.85 (+/- 0.13) [KNN]
    ```

  로지스틱 회귀와 k-최근접 이웃 알고리즘은 결정트리와 다르게 스케일에 민감하다. 붓꽃 데이터셋의 특성은 모두 같은 스케일로 측정되었지만 특성을 표준화 전처리하는 것은 좋은 습관이다.

- 다수결 투표 앙상블을 위해 MajorityVoteClassifier 클래스로 각 분류기를 하나로 연결한다.

  ```python
  mv_clf = MajorityVoteClassifier(classifiers=[pipe1, clf2, pipe3])
  
  clf_labels += ['Majority voting']
  all_clf = [pipe1, clf2, pipe3, mv_clf]
  
  for clf, label in zip(all_clf, clf_labels):
      scores = cross_val_score(estimator=clf,
                               X=X_train,
                               y=y_train,
                               cv=10,
                               scoring='roc_auc')
      print("ROC AUC: %0.2f (+/- %0.2f) [%s]"
            % (scores.mean(), scores.std(), label))
  ```

  - 결과

    ```
    ROC AUC: 0.92 (+/- 0.15) [Logistic regression]
    ROC AUC: 0.87 (+/- 0.18) [Decision tree]
    ROC AUC: 0.85 (+/- 0.13) [KNN]
    ROC AUC: 0.98 (+/- 0.05) [Majority voting]
    ```

  10-겹 교차 검증으로 평가했을 때의 개별 분류기 성능보다 MajorityVoteClassifier의 성능이 뛰어나다.



